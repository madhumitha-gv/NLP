{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13502740,"sourceType":"datasetVersion","datasetId":8573070}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#a fresh Python environment\n!pip install --force-reinstall --no-cache-dir numpy==1.24.3 scipy==1.10.1 gensim==4.3.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T15:50:56.755351Z","iopub.execute_input":"2025-10-26T15:50:56.755908Z","iopub.status.idle":"2025-10-26T15:51:39.886124Z","shell.execute_reply.started":"2025-10-26T15:50:56.755885Z","shell.execute_reply":"2025-10-26T15:51:39.885203Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy==1.24.3\n  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nCollecting scipy==1.10.1\n  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting gensim==4.3.0\n  Downloading gensim-4.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\nCollecting smart-open>=1.8.1 (from gensim==4.3.0)\n  Downloading smart_open-7.4.1-py3-none-any.whl.metadata (24 kB)\nCollecting FuzzyTM>=0.4.0 (from gensim==4.3.0)\n  Downloading FuzzyTM-2.0.9-py3-none-any.whl.metadata (7.9 kB)\nCollecting pandas (from FuzzyTM>=0.4.0->gensim==4.3.0)\n  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m290.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyfume (from FuzzyTM>=0.4.0->gensim==4.3.0)\n  Downloading pyFUME-0.3.4-py3-none-any.whl.metadata (9.7 kB)\nCollecting wrapt (from smart-open>=1.8.1->gensim==4.3.0)\n  Downloading wrapt-2.0.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (8.8 kB)\nCollecting python-dateutil>=2.8.2 (from pandas->FuzzyTM>=0.4.0->gensim==4.3.0)\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\nCollecting pytz>=2020.1 (from pandas->FuzzyTM>=0.4.0->gensim==4.3.0)\n  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting tzdata>=2022.7 (from pandas->FuzzyTM>=0.4.0->gensim==4.3.0)\n  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\nINFO: pip is looking at multiple versions of pyfume to determine which version is compatible with other requirements. This could take a while.\nCollecting pyfume (from FuzzyTM>=0.4.0->gensim==4.3.0)\n  Downloading pyFUME-0.3.1-py3-none-any.whl.metadata (9.7 kB)\nCollecting simpful (from pyfume->FuzzyTM>=0.4.0->gensim==4.3.0)\n  Downloading simpful-2.12.0-py3-none-any.whl.metadata (4.8 kB)\nCollecting fst-pso (from pyfume->FuzzyTM>=0.4.0->gensim==4.3.0)\n  Downloading fst_pso-1.9.0-py3-none-any.whl.metadata (3.2 kB)\nCollecting typing-extensions (from pyfume->FuzzyTM>=0.4.0->gensim==4.3.0)\n  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting six>=1.5 (from python-dateutil>=2.8.2->pandas->FuzzyTM>=0.4.0->gensim==4.3.0)\n  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\nCollecting miniful (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim==4.3.0)\n  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nDownloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading gensim-4.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m188.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading FuzzyTM-2.0.9-py3-none-any.whl (31 kB)\nDownloading smart_open-7.4.1-py3-none-any.whl (63 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m263.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m192.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyFUME-0.3.1-py3-none-any.whl (59 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m290.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading wrapt-2.0.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (114 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.1/114.1 kB\u001b[0m \u001b[31m307.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m338.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m352.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m361.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fst_pso-1.9.0-py3-none-any.whl (25 kB)\nDownloading simpful-2.12.0-py3-none-any.whl (24 kB)\nDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m251.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nBuilding wheels for collected packages: miniful\n  Building wheel for miniful (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3506 sha256=fc7df7b271a9d1779ea2c397b494fc7f30a2dd705dced74e7e2c99bb101d2f16\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ns_8im5o/wheels/9d/ff/2f/afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\nSuccessfully built miniful\nInstalling collected packages: pytz, wrapt, tzdata, typing-extensions, six, numpy, smart-open, scipy, python-dateutil, simpful, pandas, miniful, fst-pso, pyfume, FuzzyTM, gensim\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2025.2\n    Uninstalling pytz-2025.2:\n      Successfully uninstalled pytz-2025.2\n  Attempting uninstall: wrapt\n    Found existing installation: wrapt 1.17.2\n    Uninstalling wrapt-1.17.2:\n      Successfully uninstalled wrapt-1.17.2\n  Attempting uninstall: tzdata\n    Found existing installation: tzdata 2025.2\n    Uninstalling tzdata-2025.2:\n      Successfully uninstalled tzdata-2025.2\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.15.0\n    Uninstalling typing_extensions-4.15.0:\n      Successfully uninstalled typing_extensions-4.15.0\n  Attempting uninstall: six\n    Found existing installation: six 1.17.0\n    Uninstalling six-1.17.0:\n      Successfully uninstalled six-1.17.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: smart-open\n    Found existing installation: smart_open 7.3.0.post1\n    Uninstalling smart_open-7.3.0.post1:\n      Successfully uninstalled smart_open-7.3.0.post1\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.15.3\n    Uninstalling scipy-1.15.3:\n      Successfully uninstalled scipy-1.15.3\n  Attempting uninstall: python-dateutil\n    Found existing installation: python-dateutil 2.9.0.post0\n    Uninstalling python-dateutil-2.9.0.post0:\n      Successfully uninstalled python-dateutil-2.9.0.post0\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.3\n    Uninstalling pandas-2.2.3:\n      Successfully uninstalled pandas-2.2.3\n  Attempting uninstall: gensim\n    Found existing installation: gensim 4.3.3\n    Uninstalling gensim-4.3.3:\n      Successfully uninstalled gensim-4.3.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\ndeprecated 1.2.18 requires wrapt<2,>=1.10, but you have wrapt 2.0.0 which is incompatible.\nbayesian-optimization 3.1.0 requires numpy>=1.25; python_full_version < \"3.13\", but you have numpy 1.24.3 which is incompatible.\nmne 1.10.1 requires numpy<3,>=1.25, but you have numpy 1.24.3 which is incompatible.\nmne 1.10.1 requires scipy>=1.11, but you have scipy 1.10.1 which is incompatible.\nkaggle-environments 1.18.0 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.24.3 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\njax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\njax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\ntokenizers 0.21.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 1.0.0rc2 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\nscikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\ntreescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\ncvxpy 1.6.7 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nalbumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nxarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\nxarray-einstats 0.9.1 requires scipy>=1.11, but you have scipy 1.10.1 which is incompatible.\nxarray 2025.7.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\ntransformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nalbucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\njaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\njaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\npymc 5.25.1 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nblosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed FuzzyTM-2.0.9 fst-pso-1.9.0 gensim-4.3.0 miniful-0.0.6 numpy-1.24.3 pandas-2.3.3 pyfume-0.3.1 python-dateutil-2.9.0.post0 pytz-2025.2 scipy-1.10.1 simpful-2.12.0 six-1.17.0 smart-open-7.4.1 typing-extensions-4.15.0 tzdata-2025.2 wrapt-2.0.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Installed required packages","metadata":{}},{"cell_type":"markdown","source":" ## IMPORTS & SETUP","metadata":{}},{"cell_type":"code","source":"import re, numpy as np, pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nimport torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:18:36.303580Z","iopub.execute_input":"2025-10-26T04:18:36.304360Z","iopub.status.idle":"2025-10-26T04:18:39.023010Z","shell.execute_reply.started":"2025-10-26T04:18:36.304337Z","shell.execute_reply":"2025-10-26T04:18:39.022126Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Defining path, and random seed for randomization","metadata":{}},{"cell_type":"code","source":"CSV_PATH = \"/kaggle/input/reviews/reviews.csv\"\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:18:39.129729Z","iopub.execute_input":"2025-10-26T04:18:39.130173Z","iopub.status.idle":"2025-10-26T04:18:39.191000Z","shell.execute_reply.started":"2025-10-26T04:18:39.130150Z","shell.execute_reply":"2025-10-26T04:18:39.190311Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Loading reviews dataset, spliting the dataset as train, test, randomly, 80-20 split","metadata":{}},{"cell_type":"code","source":"print(\"Loading data...\")\ndf = pd.read_csv(CSV_PATH).dropna(subset=[\"Text\"]).reset_index(drop=True)\nX_all = df[\"Text\"].astype(str).tolist()\ny_all = df[\"Sentiment\"].astype(int).to_numpy()\n\nX_train_text, X_test_text, y_train, y_test = train_test_split(\n    X_all, y_all, test_size=0.2, random_state=SEED, stratify=y_all\n)\nprint(f\"✓ {len(X_train_text)} train | {len(X_test_text)} test\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:18:40.805262Z","iopub.execute_input":"2025-10-26T04:18:40.805914Z","iopub.status.idle":"2025-10-26T04:18:41.557050Z","shell.execute_reply.started":"2025-10-26T04:18:40.805887Z","shell.execute_reply":"2025-10-26T04:18:41.556165Z"}},"outputs":[{"name":"stdout","text":"Loading data...\n✓ 40000 train | 10000 test\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Simple neural network using pytorch","metadata":{}},{"cell_type":"code","source":"class SimpleNN(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.dropout = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(128, 1)\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = torch.sigmoid(self.fc2(x)).squeeze(1)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:18:43.196528Z","iopub.execute_input":"2025-10-26T04:18:43.197254Z","iopub.status.idle":"2025-10-26T04:18:43.202417Z","shell.execute_reply.started":"2025-10-26T04:18:43.197226Z","shell.execute_reply":"2025-10-26T04:18:43.201530Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Training function","metadata":{}},{"cell_type":"code","source":"def train_model(X_train, X_test, y_train, y_test):\n    \"\"\"Train and evaluate model\"\"\"\n    # Convert to tensors\n    X_tr = torch.tensor(X_train, dtype=torch.float32)\n    X_te = torch.tensor(X_test, dtype=torch.float32)\n    y_tr = torch.tensor(y_train, dtype=torch.float32)\n    y_te = torch.tensor(y_test, dtype=torch.float32)\n    \n    # Setup model\n    model = SimpleNN(X_train.shape[1]).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    loss_fn = nn.BCELoss()\n    \n    # Training\n    train_loader = DataLoader(TensorDataset(X_tr, y_tr), batch_size=64, shuffle=True)\n    model.train()\n    for epoch in range(6):\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            predictions = model(X_batch)\n            loss = loss_fn(predictions, y_batch)\n            loss.backward()\n            optimizer.step()\n    \n    # Evaluation\n    model.eval()\n    with torch.no_grad():\n        predictions = model(X_te.to(device)).cpu().numpy()\n    y_pred = (predictions > 0.5).astype(int)\n    \n    acc = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    return acc, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:18:45.273807Z","iopub.execute_input":"2025-10-26T04:18:45.274716Z","iopub.status.idle":"2025-10-26T04:18:45.281560Z","shell.execute_reply.started":"2025-10-26T04:18:45.274687Z","shell.execute_reply":"2025-10-26T04:18:45.280699Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Count Vectorizer","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"METHOD 1: COUNT VECTORIZER\")\nprint(\"=\" * 60)\n\nvectorizer = CountVectorizer(max_features=5000, ngram_range=(1,1))\nX_train_counts = vectorizer.fit_transform(X_train_text)\nX_test_counts = vectorizer.transform(X_test_text)\n\nprint(f\"Original shape - Train: {X_train_counts.shape}, Test: {X_test_counts.shape}\")\n# Reduce dimensions with SVD\nsvd = TruncatedSVD(n_components=300, random_state=SEED)\nX_train_svd = svd.fit_transform(X_train_counts)\nX_test_svd = svd.transform(X_test_counts)\n\nprint(f\"After SVD - Train: {X_train_svd.shape}, Test: {X_test_svd.shape}\")\n\n# Scale\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_svd).astype(np.float32)\nX_test_scaled = scaler.transform(X_test_svd).astype(np.float32)\n\nprint(f\" Final shape: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n\nacc, f1 = train_model(X_train_scaled, X_test_scaled, y_train, y_test)\nprint(f\"Accuracy: {acc:.4f} | F1: {f1:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:18:48.563460Z","iopub.execute_input":"2025-10-26T04:18:48.564005Z","iopub.status.idle":"2025-10-26T04:19:27.448148Z","shell.execute_reply.started":"2025-10-26T04:18:48.563980Z","shell.execute_reply":"2025-10-26T04:19:27.447219Z"}},"outputs":[{"name":"stdout","text":"============================================================\nMETHOD 1: COUNT VECTORIZER\n============================================================\nAccuracy: 0.8566 | F1: 0.8571\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Term Frequency Inverse Document Frequency(TF-IDF)","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"METHOD 2: TF-IDF\")\nprint(\"=\" * 60)\n\nvectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2), stop_words=\"english\")\nX_train_tfidf = vectorizer.fit_transform(X_train_text)\nX_test_tfidf = vectorizer.transform(X_test_text)\n\nprint(f\"Original shape - Train: {X_train_tfidf.shape}, Test: {X_test_tfidf.shape}\")\n# Reduce dimensions with SVD\nsvd = TruncatedSVD(n_components=300, random_state=SEED)\nX_train_svd = svd.fit_transform(X_train_tfidf)\nX_test_svd = svd.transform(X_test_tfidf)\nprint(f\"After SVD - Train: {X_train_svd.shape}, Test: {X_test_svd.shape}\")\n\n# Scale\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_svd).astype(np.float32)\nX_test_scaled = scaler.transform(X_test_svd).astype(np.float32)\n\nacc, f1 = train_model(X_train_scaled, X_test_scaled, y_train, y_test)\nprint(f\" Final shape: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\nprint(f\"Accuracy: {acc:.4f} | F1: {f1:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:21:07.063470Z","iopub.execute_input":"2025-10-26T04:21:07.063822Z","iopub.status.idle":"2025-10-26T04:21:55.656130Z","shell.execute_reply.started":"2025-10-26T04:21:07.063801Z","shell.execute_reply":"2025-10-26T04:21:55.655270Z"}},"outputs":[{"name":"stdout","text":"============================================================\nMETHOD 2: TF-IDF\n============================================================\nAccuracy: 0.8782 | F1: 0.8786\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Word2Vec","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"METHOD 3: WORD2VEC (trained on your data)\")\nprint(\"=\" * 60)\n\nfrom gensim.models import Word2Vec\n\ndef tokenize(text):\n    \"\"\"Simple tokenizer\"\"\"\n    return re.findall(r\"[a-z']+\", text.lower())\n\n# Train Word2Vec on training data\ntokenized_train = [tokenize(text) for text in X_train_text]\nw2v_model = Word2Vec(sentences=tokenized_train, vector_size=300, window=5, \n                     min_count=2, sg=1, epochs=10, workers=4)\nprint(f\"Model trained - Vocabulary size: {len(w2v_model.wv)}\")\n\ndef document_vector(text, model):\n    \"\"\"Average word vectors for a document\"\"\"\n    tokens = tokenize(text)\n    vectors = [model.wv[word] for word in tokens if word in model.wv]\n    if len(vectors) > 0:\n        return np.mean(vectors, axis=0)\n    else:\n        return np.zeros(300)\n\nX_train_w2v = np.vstack([document_vector(text, w2v_model) for text in X_train_text]).astype(np.float32)\nX_test_w2v = np.vstack([document_vector(text, w2v_model) for text in X_test_text]).astype(np.float32)\n\nprint(f\" Final shape: {X_train_w2v.shape}, Test: {X_test_w2v.shape}\")\n\nacc, f1 = train_model(X_train_w2v, X_test_w2v, y_train, y_test)\nprint(f\"Accuracy: {acc:.4f} | F1: {f1:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:29:50.647813Z","iopub.execute_input":"2025-10-26T04:29:50.648093Z","iopub.status.idle":"2025-10-26T04:36:33.272949Z","shell.execute_reply.started":"2025-10-26T04:29:50.648071Z","shell.execute_reply":"2025-10-26T04:36:33.271915Z"}},"outputs":[{"name":"stdout","text":"============================================================\nMETHOD 3: WORD2VEC (trained on your data)\n============================================================\nAccuracy: 0.8771 | F1: 0.8796\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## GloVe","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"METHOD 4: GLOVE (pretrained embeddings)\")\nprint(\"=\" * 60)\n\nimport gensim.downloader as api\n\nprint(\"Downloading GloVe model (this may take a minute)...\")\nglove_model = api.load(\"glove-wiki-gigaword-100\")\nprint(f\"Model loaded - Vocabulary size: {len(glove_model)}\")\ndef document_vector_glove(text, model):\n    \"\"\"Average GloVe vectors for a document\"\"\"\n    tokens = tokenize(text)\n    vectors = [model[word] for word in tokens if word in model]\n    if len(vectors) > 0:\n        return np.mean(vectors, axis=0)\n    else:\n        return np.zeros(100)\n\nX_train_glove = np.vstack([document_vector_glove(text, glove_model) for text in X_train_text]).astype(np.float32)\nX_test_glove = np.vstack([document_vector_glove(text, glove_model) for text in X_test_text]).astype(np.float32)\n\n\nprint(f\" Final shape: {X_train_glove.shape}, Test: {X_test_glove.shape}\")\nacc, f1 = train_model(X_train_glove, X_test_glove, y_train, y_test)\nprint(f\"Accuracy: {acc:.4f} | F1: {f1:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:38:33.961240Z","iopub.execute_input":"2025-10-26T04:38:33.961783Z","iopub.status.idle":"2025-10-26T04:39:28.643546Z","shell.execute_reply.started":"2025-10-26T04:38:33.961757Z","shell.execute_reply":"2025-10-26T04:39:28.642485Z"}},"outputs":[{"name":"stdout","text":"============================================================\nMETHOD 4: GLOVE (pretrained embeddings)\n============================================================\nDownloading GloVe model (this may take a minute)...\nAccuracy: 0.7896 | F1: 0.7990\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## FastText","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"METHOD 5: FASTTEXT (pretrained embeddings)\")\nprint(\"=\" * 60)\n\nprint(\"Downloading FastText model (this may take a few minutes)...\")\nfasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\nprint(f\"Model loaded - Vocabulary size: {len(fasttext_model)}\")\n\ndef document_vector_fasttext(text, model):\n    \"\"\"Average FastText vectors for a document\"\"\"\n    tokens = tokenize(text)\n    vectors = [model[word] for word in tokens if word in model]\n    if len(vectors) > 0:\n        return np.mean(vectors, axis=0)\n    else:\n        return np.zeros(300)\n\nX_train_ft = np.vstack([document_vector_fasttext(text, fasttext_model) for text in X_train_text]).astype(np.float32)\nX_test_ft = np.vstack([document_vector_fasttext(text, fasttext_model) for text in X_test_text]).astype(np.float32)\n\nprint(f\" Final shape: {X_train_ft.shape}, Test: {X_test_ft.shape}\")\nacc, f1 = train_model(X_train_ft, X_test_ft, y_train, y_test)\nprint(f\"Accuracy: {acc:.4f} | F1: {f1:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:41:37.201444Z","iopub.execute_input":"2025-10-26T04:41:37.202023Z","iopub.status.idle":"2025-10-26T04:45:05.415000Z","shell.execute_reply.started":"2025-10-26T04:41:37.201996Z","shell.execute_reply":"2025-10-26T04:45:05.414108Z"}},"outputs":[{"name":"stdout","text":"============================================================\nMETHOD 5: FASTTEXT (pretrained embeddings)\n============================================================\nDownloading FastText model (this may take a few minutes)...\nAccuracy: 0.8440 | F1: 0.8404\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"SUMMARY OF RESULTS\")\nprint(\"=\" * 60)\nprint(\"\\nAll methods tested with same simple architecture:\")\nprint(\"- 1 hidden layer (128 neurons)\")\nprint(\"- 0.3 dropout\")\nprint(\"- 6 epochs\")\nprint(\"- Learning rate: 0.001\")\nprint(\"\\n\")\nprint(\"\\nNext step: Take the best embedding and try different architectures.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:47:23.530123Z","iopub.execute_input":"2025-10-26T04:47:23.530425Z","iopub.status.idle":"2025-10-26T04:47:23.536124Z","shell.execute_reply.started":"2025-10-26T04:47:23.530404Z","shell.execute_reply":"2025-10-26T04:47:23.535264Z"}},"outputs":[{"name":"stdout","text":"============================================================\nSUMMARY OF RESULTS\n============================================================\n\nAll methods tested with same simple architecture:\n- 1 hidden layer (128 neurons)\n- 0.3 dropout\n- 6 epochs\n- Learning rate: 0.001\n\nCheck the accuracy & F1 scores above to see which embedding works best!\n\nNext step: Take the best embedding and try different architectures.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Testing out different NN architectures usign Word2Vec Embeddings","metadata":{}},{"cell_type":"markdown","source":"### Defining neural network models","metadata":{}},{"cell_type":"markdown","source":"### Simple NN we used before is considered as baseline\n    \n*  1 hidden layer (128 neurons) 128->128->1\n*  Simple dropout\n*  Total parameters: ~40K\n","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# NEURAL NETWORK IMPROVEMENT EXPERIMENTS\n\nimport time\nclass BaselineNN(nn.Module):\n    \"\"\"\n    BASELINE MODEL\n    - 1 hidden layer (128 neurons)\n    - Simple dropout\n    - Total parameters: ~40K\n    \"\"\"\n    def __init__(self, input_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.dropout = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(128, 1)\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        return torch.sigmoid(self.fc2(x)).squeeze(1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:57:37.254739Z","iopub.execute_input":"2025-10-26T04:57:37.255228Z","iopub.status.idle":"2025-10-26T04:57:37.272427Z","shell.execute_reply.started":"2025-10-26T04:57:37.255206Z","shell.execute_reply":"2025-10-26T04:57:37.271637Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### Deeper network with more hidden layers \n* 3 hidden layers (256 -> 128 -> 64->1)\n* Batch normalization added\n*  Total parameters: ~110K","metadata":{}},{"cell_type":"code","source":"class DeeperNN(nn.Module):\n    \"\"\"\n    EXPERIMENT 1: DEEPER NETWORK\n    - 3 hidden layers (256 -> 128 -> 64)\n    - Batch normalization added\n    - Total parameters: ~110K\n\n    \"\"\"\n    def __init__(self, input_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.dropout1 = nn.Dropout(0.3)\n        \n        self.fc2 = nn.Linear(256, 128)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.dropout2 = nn.Dropout(0.3)\n        \n        self.fc3 = nn.Linear(128, 64)\n        self.bn3 = nn.BatchNorm1d(64)\n        self.dropout3 = nn.Dropout(0.2)\n        \n        self.fc4 = nn.Linear(64, 1)\n    \n    def forward(self, x):\n        x = torch.relu(self.bn1(self.fc1(x)))\n        x = self.dropout1(x)\n        x = torch.relu(self.bn2(self.fc2(x)))\n        x = self.dropout2(x)\n        x = torch.relu(self.bn3(self.fc3(x)))\n        x = self.dropout3(x)\n        return torch.sigmoid(self.fc4(x)).squeeze(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Wider Neural network\n* 2 hidden layers (512 -> 256)\n*  More neurons per layer\n*  Total parameters: ~290K","metadata":{}},{"cell_type":"code","source":"class WiderNN(nn.Module):\n    \"\"\"\n    EXPERIMENT 2: WIDER NETWORK\n    - 2 hidden layers (512 -> 256->1)\n    - More neurons per layer\n    - Total parameters: ~290K\n    \n    HYPOTHESIS: More neurons = more capacity to learn complex patterns\n    \"\"\"\n    def __init__(self, input_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.dropout1 = nn.Dropout(0.4)\n        \n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.dropout2 = nn.Dropout(0.3)  \n        self.fc3 = nn.Linear(256, 1)\n    \n    def forward(self, x):\n        x = torch.relu(self.bn1(self.fc1(x)))\n        x = self.dropout1(x)\n        x = torch.relu(self.bn2(self.fc2(x)))\n        x = self.dropout2(x)\n        return torch.sigmoid(self.fc3(x)).squeeze(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Training Function","metadata":{}},{"cell_type":"code","source":"\ndef train_model(X_train, X_test, y_train, y_test, \n                model_class, epochs, lr, batch_size, name):\n    \"\"\"Train and evaluate a model\"\"\"\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"{name}\")\n    print(f\"{'='*80}\")\n    print(f\"Hyperparameters:\")\n    print(f\"  - Epochs: {epochs}\")\n    print(f\"  - Learning Rate: {lr}\")\n    print(f\"  - Batch Size: {batch_size}\")\n    \n    start_time = time.time()\n    \n    # Prepare data\n    X_tr = torch.tensor(X_train, dtype=torch.float32)\n    X_te = torch.tensor(X_test, dtype=torch.float32)\n    y_tr = torch.tensor(y_train, dtype=torch.float32)\n    y_te = torch.tensor(y_test, dtype=torch.float32)\n    \n    # Initialize model\n    model = model_class(X_train.shape[1]).to(device)\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"  - Parameters: {num_params:,}\")\n    \n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    loss_fn = nn.BCELoss()\n    train_loader = DataLoader(TensorDataset(X_tr, y_tr), batch_size=batch_size, shuffle=True)\n    \n    # Training loop\n    print(f\"\\nTraining...\")\n    losses = []\n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0.0\n        \n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            predictions = model(X_batch)\n            loss = loss_fn(predictions, y_batch)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        \n        avg_loss = epoch_loss / len(train_loader)\n        losses.append(avg_loss)\n        \n        # Print progress\n        if epoch == 0 or (epoch + 1) % 5 == 0:\n            print(f\"  Epoch {epoch+1:2d}/{epochs} - Loss: {avg_loss:.4f}\")\n    \n    # Evaluation\n    model.eval()\n    with torch.no_grad():\n        predictions = model(X_te.to(device)).cpu().numpy()\n    y_pred = (predictions > 0.5).astype(int)\n    \n    acc = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    training_time = time.time() - start_time\n    \n    print(f\"\\n{'Results:'}\")\n    print(f\"  ✓ Accuracy: {acc:.4f}\")\n    print(f\"  ✓ F1 Score: {f1:.4f}\")\n    print(f\"  ✓ Training Time: {training_time:.2f}s\")\n    \n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'time': training_time,\n        'losses': losses,\n        'params': num_params,\n        'name': name\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T15:13:54.160724Z","iopub.execute_input":"2025-10-26T15:13:54.161297Z","iopub.status.idle":"2025-10-26T15:13:54.172354Z","shell.execute_reply.started":"2025-10-26T15:13:54.161274Z","shell.execute_reply":"2025-10-26T15:13:54.171489Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## tes","metadata":{}},{"cell_type":"markdown","source":"### simple NN architecture is considered as baseline, we see word2Vec embeddings gave better accuracy, so for simplicity using that embeddings for testing different NN models","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"RUNNING EXPERIMENTS\")\nprint(\"=\"*80)\n\nresults = []\n\n# BASELINE\nresults.append(train_model(\n    X_train_w2v, X_test_w2v, y_train, y_test,\n    model_class=BaselineNN,\n    epochs=10,\n    lr=0.001,\n    batch_size=64,\n    name=\"BASELINE: Simple Network (1 layer, 128 neurons)\"\n))\n\n# EXPERIMENT 1: Deeper Network\nresults.append(train_model(\n    X_train_w2v, X_test_w2v, y_train, y_test,\n    model_class=DeeperNN,\n    epochs=10,\n    lr=0.001,\n    batch_size=64,\n    name=\"EXPERIMENT 1: Deeper Network (3 layers: 256→128→64)\"\n))\n\n# EXPERIMENT 2: Wider Network\nresults.append(train_model(\n    X_train_w2v, X_test_w2v, y_train, y_test,\n    model_class=WiderNN,\n    epochs=10,\n    lr=0.001,\n    batch_size=64,\n    name=\"EXPERIMENT 2: Wider Network (2 layers: 512→256)\"\n))\n\n# EXPERIMENT 3: Optimized Hyperparameters (using deeper network)\nresults.append(train_model(\n    X_train_w2v, X_test_w2v, y_train, y_test,\n    model_class=DeeperNN,\n    epochs=10,  # More epochs\n    lr=0.0005,  # Lower learning rate\n    batch_size=64,\n    name=\"EXPERIMENT 3: Deeper Network + Optimized Hyperparameters (20 epochs, LR=0.0005)\"\n))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T05:29:56.704658Z","iopub.execute_input":"2025-10-26T05:29:56.705014Z","iopub.status.idle":"2025-10-26T05:31:07.208075Z","shell.execute_reply.started":"2025-10-26T05:29:56.704995Z","shell.execute_reply":"2025-10-26T05:31:07.207153Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nRUNNING EXPERIMENTS\n================================================================================\n\n================================================================================\nBASELINE: Simple Network (1 layer, 128 neurons)\n================================================================================\nHyperparameters:\n  - Epochs: 10\n  - Learning Rate: 0.001\n  - Batch Size: 64\n  - Parameters: 38,657\n\nTraining...\n  Epoch  1/10 - Loss: 0.4591\n  Epoch  5/10 - Loss: 0.3013\n  Epoch 10/10 - Loss: 0.2883\n\nResults:\n  ✓ Accuracy: 0.8803\n  ✓ F1 Score: 0.8775\n  ✓ Training Time: 13.03s\n\n================================================================================\nEXPERIMENT 1: Deeper Network (3 layers: 256→128→64)\n================================================================================\nHyperparameters:\n  - Epochs: 10\n  - Learning Rate: 0.001\n  - Batch Size: 64\n  - Parameters: 119,169\n\nTraining...\n  Epoch  1/10 - Loss: 0.3206\n  Epoch  5/10 - Loss: 0.2532\n  Epoch 10/10 - Loss: 0.2185\n\nResults:\n  ✓ Accuracy: 0.8907\n  ✓ F1 Score: 0.8938\n  ✓ Training Time: 20.08s\n\n================================================================================\nEXPERIMENT 2: Wider Network (2 layers: 512→256)\n================================================================================\nHyperparameters:\n  - Epochs: 10\n  - Learning Rate: 0.001\n  - Batch Size: 64\n  - Parameters: 287,233\n\nTraining...\n  Epoch  1/10 - Loss: 0.3129\n  Epoch  5/10 - Loss: 0.2523\n  Epoch 10/10 - Loss: 0.2119\n\nResults:\n  ✓ Accuracy: 0.8820\n  ✓ F1 Score: 0.8767\n  ✓ Training Time: 17.06s\n\n================================================================================\nEXPERIMENT 3: Deeper Network + Optimized Hyperparameters (20 epochs, LR=0.0005)\n================================================================================\nHyperparameters:\n  - Epochs: 10\n  - Learning Rate: 0.0005\n  - Batch Size: 64\n  - Parameters: 119,169\n\nTraining...\n  Epoch  1/10 - Loss: 0.3355\n  Epoch  5/10 - Loss: 0.2529\n  Epoch 10/10 - Loss: 0.2180\n\nResults:\n  ✓ Accuracy: 0.8927\n  ✓ F1 Score: 0.8941\n  ✓ Training Time: 20.30s\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# EXPERIMENT 4: Optimized Hyperparameters (using deeper network), increased epochs\nresults.append(train_model(\n    X_train_w2v, X_test_w2v, y_train, y_test,\n    model_class=DeeperNN,\n    epochs=100,  # More epochs\n    lr=0.001,  # Lower learning rate\n    batch_size=64,\n    name=\"EXPERIMENT 4: Deeper Network + Optimized Hyperparameters (20 epochs, LR=0.0001)\"\n))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T05:31:07.209342Z","iopub.execute_input":"2025-10-26T05:31:07.209778Z","iopub.status.idle":"2025-10-26T05:34:28.211491Z","shell.execute_reply.started":"2025-10-26T05:31:07.209744Z","shell.execute_reply":"2025-10-26T05:34:28.210562Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nEXPERIMENT 3: Deeper Network + Optimized Hyperparameters (20 epochs, LR=0.0005)\n================================================================================\nHyperparameters:\n  - Epochs: 100\n  - Learning Rate: 0.001\n  - Batch Size: 64\n  - Parameters: 119,169\n\nTraining...\n  Epoch  1/100 - Loss: 0.3218\n  Epoch  5/100 - Loss: 0.2553\n  Epoch 10/100 - Loss: 0.2212\n  Epoch 15/100 - Loss: 0.1835\n  Epoch 20/100 - Loss: 0.1575\n  Epoch 25/100 - Loss: 0.1387\n  Epoch 30/100 - Loss: 0.1212\n  Epoch 35/100 - Loss: 0.1104\n  Epoch 40/100 - Loss: 0.1025\n  Epoch 45/100 - Loss: 0.0903\n  Epoch 50/100 - Loss: 0.0868\n  Epoch 55/100 - Loss: 0.0842\n  Epoch 60/100 - Loss: 0.0753\n  Epoch 65/100 - Loss: 0.0727\n  Epoch 70/100 - Loss: 0.0695\n  Epoch 75/100 - Loss: 0.0678\n  Epoch 80/100 - Loss: 0.0670\n  Epoch 85/100 - Loss: 0.0635\n  Epoch 90/100 - Loss: 0.0590\n  Epoch 95/100 - Loss: 0.0589\n  Epoch 100/100 - Loss: 0.0544\n\nResults:\n  ✓ Accuracy: 0.8802\n  ✓ F1 Score: 0.8774\n  ✓ Training Time: 200.99s\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"### Since the basic NN architecture has better accuracy, optimzied the hyperparameters like learning rate, increased epochs, increased batch size to see if there is any better performance and yes there is slight improvement from 87% to 89%","metadata":{}},{"cell_type":"code","source":"# experiment 5 changed batch size and epochs as 100, learning rate as it is\nresults.append(train_model(\n    X_train_w2v, X_test_w2v, y_train, y_test,\n    model_class=BaselineNN,\n    epochs=100,\n    lr=0.001,\n    batch_size=128,\n    name=\"experiment 5: Simple Network (1 layer, 128 neurons) with tuned hyper parameters\"\n))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:11:12.091068Z","iopub.execute_input":"2025-10-26T06:11:12.091644Z","iopub.status.idle":"2025-10-26T06:12:32.918883Z","shell.execute_reply.started":"2025-10-26T06:11:12.091613Z","shell.execute_reply":"2025-10-26T06:12:32.918238Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nBASELINE: Simple Network (1 layer, 128 neurons)\n================================================================================\nHyperparameters:\n  - Epochs: 100\n  - Learning Rate: 0.001\n  - Batch Size: 128\n  - Parameters: 38,657\n\nTraining...\n  Epoch  1/100 - Loss: 0.5206\n  Epoch  5/100 - Loss: 0.3079\n  Epoch 10/100 - Loss: 0.2938\n  Epoch 15/100 - Loss: 0.2879\n  Epoch 20/100 - Loss: 0.2841\n  Epoch 25/100 - Loss: 0.2795\n  Epoch 30/100 - Loss: 0.2770\n  Epoch 35/100 - Loss: 0.2749\n  Epoch 40/100 - Loss: 0.2733\n  Epoch 45/100 - Loss: 0.2696\n  Epoch 50/100 - Loss: 0.2678\n  Epoch 55/100 - Loss: 0.2662\n  Epoch 60/100 - Loss: 0.2659\n  Epoch 65/100 - Loss: 0.2638\n  Epoch 70/100 - Loss: 0.2624\n  Epoch 75/100 - Loss: 0.2607\n  Epoch 80/100 - Loss: 0.2588\n  Epoch 85/100 - Loss: 0.2553\n  Epoch 90/100 - Loss: 0.2559\n  Epoch 95/100 - Loss: 0.2558\n  Epoch 100/100 - Loss: 0.2535\n\nResults:\n  ✓ Accuracy: 0.8900\n  ✓ F1 Score: 0.8910\n  ✓ Training Time: 80.82s\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"###  and accuracy has slightly improved to 89%","metadata":{}},{"cell_type":"markdown","source":"## Summary of Results","metadata":{}},{"cell_type":"code","source":"\nprint(\"\\n\\n\" + \"=\"*80)\nprint(\"FINAL RESULTS SUMMARY\")\nprint(\"=\"*80)\n\n# Create comparison table\nprint(\"\\n{:<60} {:>10} {:>10} {:>12} {:>10}\".format(\n    \"Experiment\", \"Accuracy\", \"F1 Score\", \"Parameters\", \"Time (s)\"\n))\nprint(\"-\" * 104)\n\nfor i, r in enumerate(results):\n    exp_name = \"BASELINE\" if i == 0 else f\"EXP {i}\"\n    print(\"{:<60} {:>10.4f} {:>10.4f} {:>12,} {:>10.1f}\".format(\n        exp_name, r['accuracy'], r['f1'], r['params'], r['time']\n    ))\n\n# Calculate improvements\nprint(\"\\n\" + \"=\"*80)\nprint(\"IMPROVEMENTS OVER BASELINE\")\nprint(\"=\"*80)\n\nbaseline = results[0]\nfor i in range(1, len(results)):\n    r = results[i]\n    acc_imp = ((r['accuracy'] - baseline['accuracy']) / baseline['accuracy']) * 100\n    f1_imp = ((r['f1'] - baseline['f1']) / baseline['f1']) * 100\n    \n    print(f\"\\nEXPERIMENT {i}:\")\n    print(f\"  Accuracy: {acc_imp:+.2f}% {'✓' if acc_imp > 0 else '✗'}\")\n    print(f\"  F1 Score: {f1_imp:+.2f}% {'✓' if f1_imp > 0 else '✗'}\")\n    print(f\"  Extra Time: +{r['time'] - baseline['time']:.1f}s\")\n\n# Find best model\nbest = max(results, key=lambda x: x['f1'])\nbest_idx = results.index(best)\nprint(f\"\\n BEST MODEL: {'BASELINE' if best_idx == 0 else f'EXPERIMENT {best_idx}'}\")\nprint(f\"   Accuracy: {best['accuracy']:.4f}\")\nprint(f\"   F1 Score: {best['f1']:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualization of F1 scores, training loss","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nprint(\"\\n\" + \"=\"*80)\nprint(\"CREATING VISUALIZATIONS\")\nprint(\"=\"*80)\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle('Neural Network Architecture Experiments - Results Comparison', \n             fontsize=16, fontweight='bold')\n\nnames = ['BASELINE', 'EXP 1\\n(Deeper)', 'EXP 2\\n(Wider)', 'EXP 3\\n(Optimized)', 'EXP4','EXP5(baseline optimized)']\ncolors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4','#FFB347','#CDB4DB']\n\n# Plot 1: Accuracy Comparison\nax = axes[0, 0]\naccuracies = [r['accuracy'] for r in results]\nbars = ax.bar(names, accuracies, color=colors, edgecolor='black', linewidth=1.5, alpha=0.8)\nax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\nax.set_title('Accuracy Comparison', fontsize=13, fontweight='bold')\nax.set_ylim([min(accuracies) - 0.01, max(accuracies) + 0.01])\nax.axhline(y=baseline['accuracy'], color='red', linestyle='--', linewidth=2, label='Baseline')\nax.grid(axis='y', alpha=0.3, linestyle='--')\nax.legend()\n\n# Add value labels\nfor bar in bars:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n\n# Plot 2: F1 Score Comparison\nax = axes[0, 1]\nf1_scores = [r['f1'] for r in results]\nbars = ax.bar(names, f1_scores, color=colors, edgecolor='black', linewidth=1.5, alpha=0.8)\nax.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\nax.set_title('F1 Score Comparison', fontsize=13, fontweight='bold')\nax.set_ylim([min(f1_scores) - 0.01, max(f1_scores) + 0.01])\nax.axhline(y=baseline['f1'], color='red', linestyle='--', linewidth=2, label='Baseline')\nax.grid(axis='y', alpha=0.3, linestyle='--')\nax.legend()\n\n# Add value labels\nfor bar in bars:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n\n# Plot 3: Training Loss Curves\nax = axes[1, 0]\nfor i, r in enumerate(results):\n    ax.plot(r['losses'], label=names[i], marker='o', linewidth=2, \n            markersize=4, color=colors[i])\nax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\nax.set_ylabel('Loss', fontsize=12, fontweight='bold')\nax.set_title('Training Loss Over Epochs', fontsize=13, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3, linestyle='--')\n\n# Plot 4: Model Complexity vs Performance\nax = axes[1, 1]\nparams = [r['params'] for r in results]\nscatter = ax.scatter(params, f1_scores, s=200, c=colors, edgecolor='black', \n                     linewidth=2, alpha=0.8, zorder=3)\nax.set_xlabel('Number of Parameters', fontsize=12, fontweight='bold')\nax.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\nax.set_title('Model Complexity vs Performance', fontsize=13, fontweight='bold')\nax.grid(True, alpha=0.3, linestyle='--')\n\n# Add labels for each point\nfor i, (x, y) in enumerate(zip(params, f1_scores)):\n    ax.annotate(names[i], (x, y), xytext=(5, 5), textcoords='offset points',\n                fontweight='bold', fontsize=9)\n\nplt.tight_layout()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}