{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13502740,"sourceType":"datasetVersion","datasetId":8573070}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#a fresh Python environment\n!pip install --force-reinstall --no-cache-dir numpy==1.24.3 scipy==1.10.1 gensim==4.3.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Installed required packages","metadata":{}},{"cell_type":"markdown","source":" ## IMPORTS & SETUP","metadata":{}},{"cell_type":"code","source":"import re, numpy as np, pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nimport torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:18:36.303580Z","iopub.execute_input":"2025-10-26T04:18:36.304360Z","iopub.status.idle":"2025-10-26T04:18:39.023010Z","shell.execute_reply.started":"2025-10-26T04:18:36.304337Z","shell.execute_reply":"2025-10-26T04:18:39.022126Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Defining path, and random seed for randomization","metadata":{}},{"cell_type":"code","source":"CSV_PATH = \"/kaggle/input/reviews/reviews.csv\"\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:18:39.129729Z","iopub.execute_input":"2025-10-26T04:18:39.130173Z","iopub.status.idle":"2025-10-26T04:18:39.191000Z","shell.execute_reply.started":"2025-10-26T04:18:39.130150Z","shell.execute_reply":"2025-10-26T04:18:39.190311Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Loading reviews dataset, spliting the dataset as train, test, randomly, 80-20 split","metadata":{}},{"cell_type":"code","source":"print(\"Loading data...\")\ndf = pd.read_csv(CSV_PATH).dropna(subset=[\"Text\"]).reset_index(drop=True)\nX_all = df[\"Text\"].astype(str).tolist()\ny_all = df[\"Sentiment\"].astype(int).to_numpy()\n\nX_train_text, X_test_text, y_train, y_test = train_test_split(\n    X_all, y_all, test_size=0.2, random_state=SEED, stratify=y_all\n)\nprint(f\"✓ {len(X_train_text)} train | {len(X_test_text)} test\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:18:40.805262Z","iopub.execute_input":"2025-10-26T04:18:40.805914Z","iopub.status.idle":"2025-10-26T04:18:41.557050Z","shell.execute_reply.started":"2025-10-26T04:18:40.805887Z","shell.execute_reply":"2025-10-26T04:18:41.556165Z"}},"outputs":[{"name":"stdout","text":"Loading data...\n✓ 40000 train | 10000 test\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Simple neural network using pytorch","metadata":{}},{"cell_type":"code","source":"class SimpleNN(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.dropout = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(128, 1)\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = torch.sigmoid(self.fc2(x)).squeeze(1)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:18:43.196528Z","iopub.execute_input":"2025-10-26T04:18:43.197254Z","iopub.status.idle":"2025-10-26T04:18:43.202417Z","shell.execute_reply.started":"2025-10-26T04:18:43.197226Z","shell.execute_reply":"2025-10-26T04:18:43.201530Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Training function","metadata":{}},{"cell_type":"code","source":"def train_model(X_train, X_test, y_train, y_test):\n    \"\"\"Train and evaluate model\"\"\"\n    # Convert to tensors\n    X_tr = torch.tensor(X_train, dtype=torch.float32)\n    X_te = torch.tensor(X_test, dtype=torch.float32)\n    y_tr = torch.tensor(y_train, dtype=torch.float32)\n    y_te = torch.tensor(y_test, dtype=torch.float32)\n    \n    # Setup model\n    model = SimpleNN(X_train.shape[1]).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    loss_fn = nn.BCELoss()\n    \n    # Training\n    train_loader = DataLoader(TensorDataset(X_tr, y_tr), batch_size=64, shuffle=True)\n    model.train()\n    for epoch in range(6):\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            predictions = model(X_batch)\n            loss = loss_fn(predictions, y_batch)\n            loss.backward()\n            optimizer.step()\n    \n    # Evaluation\n    model.eval()\n    with torch.no_grad():\n        predictions = model(X_te.to(device)).cpu().numpy()\n    y_pred = (predictions > 0.5).astype(int)\n    \n    acc = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    return acc, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:18:45.273807Z","iopub.execute_input":"2025-10-26T04:18:45.274716Z","iopub.status.idle":"2025-10-26T04:18:45.281560Z","shell.execute_reply.started":"2025-10-26T04:18:45.274687Z","shell.execute_reply":"2025-10-26T04:18:45.280699Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Count Vectorizer","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"METHOD 1: COUNT VECTORIZER\")\nprint(\"=\" * 60)\n\nvectorizer = CountVectorizer(max_features=5000, ngram_range=(1,1))\nX_train_counts = vectorizer.fit_transform(X_train_text)\nX_test_counts = vectorizer.transform(X_test_text)\n\n# Reduce dimensions with SVD\nsvd = TruncatedSVD(n_components=300, random_state=SEED)\nX_train_svd = svd.fit_transform(X_train_counts)\nX_test_svd = svd.transform(X_test_counts)\n\n# Scale\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_svd).astype(np.float32)\nX_test_scaled = scaler.transform(X_test_svd).astype(np.float32)\n\nacc, f1 = train_model(X_train_scaled, X_test_scaled, y_train, y_test)\nprint(f\"Accuracy: {acc:.4f} | F1: {f1:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:18:48.563460Z","iopub.execute_input":"2025-10-26T04:18:48.564005Z","iopub.status.idle":"2025-10-26T04:19:27.448148Z","shell.execute_reply.started":"2025-10-26T04:18:48.563980Z","shell.execute_reply":"2025-10-26T04:19:27.447219Z"}},"outputs":[{"name":"stdout","text":"============================================================\nMETHOD 1: COUNT VECTORIZER\n============================================================\nAccuracy: 0.8566 | F1: 0.8571\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Term Frequency Inverse Document Frequency(TF-IDF)","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"METHOD 2: TF-IDF\")\nprint(\"=\" * 60)\n\nvectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2), stop_words=\"english\")\nX_train_tfidf = vectorizer.fit_transform(X_train_text)\nX_test_tfidf = vectorizer.transform(X_test_text)\n\n# Reduce dimensions with SVD\nsvd = TruncatedSVD(n_components=300, random_state=SEED)\nX_train_svd = svd.fit_transform(X_train_tfidf)\nX_test_svd = svd.transform(X_test_tfidf)\n\n# Scale\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_svd).astype(np.float32)\nX_test_scaled = scaler.transform(X_test_svd).astype(np.float32)\n\nacc, f1 = train_model(X_train_scaled, X_test_scaled, y_train, y_test)\nprint(f\"Accuracy: {acc:.4f} | F1: {f1:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:21:07.063470Z","iopub.execute_input":"2025-10-26T04:21:07.063822Z","iopub.status.idle":"2025-10-26T04:21:55.656130Z","shell.execute_reply.started":"2025-10-26T04:21:07.063801Z","shell.execute_reply":"2025-10-26T04:21:55.655270Z"}},"outputs":[{"name":"stdout","text":"============================================================\nMETHOD 2: TF-IDF\n============================================================\nAccuracy: 0.8782 | F1: 0.8786\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Word2Vec","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"METHOD 3: WORD2VEC (trained on your data)\")\nprint(\"=\" * 60)\n\nfrom gensim.models import Word2Vec\n\ndef tokenize(text):\n    \"\"\"Simple tokenizer\"\"\"\n    return re.findall(r\"[a-z']+\", text.lower())\n\n# Train Word2Vec on training data\ntokenized_train = [tokenize(text) for text in X_train_text]\nw2v_model = Word2Vec(sentences=tokenized_train, vector_size=300, window=5, \n                     min_count=2, sg=1, epochs=10, workers=4)\n\ndef document_vector(text, model):\n    \"\"\"Average word vectors for a document\"\"\"\n    tokens = tokenize(text)\n    vectors = [model.wv[word] for word in tokens if word in model.wv]\n    if len(vectors) > 0:\n        return np.mean(vectors, axis=0)\n    else:\n        return np.zeros(300)\n\nX_train_w2v = np.vstack([document_vector(text, w2v_model) for text in X_train_text]).astype(np.float32)\nX_test_w2v = np.vstack([document_vector(text, w2v_model) for text in X_test_text]).astype(np.float32)\n\nacc, f1 = train_model(X_train_w2v, X_test_w2v, y_train, y_test)\nprint(f\"Accuracy: {acc:.4f} | F1: {f1:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:29:50.647813Z","iopub.execute_input":"2025-10-26T04:29:50.648093Z","iopub.status.idle":"2025-10-26T04:36:33.272949Z","shell.execute_reply.started":"2025-10-26T04:29:50.648071Z","shell.execute_reply":"2025-10-26T04:36:33.271915Z"}},"outputs":[{"name":"stdout","text":"============================================================\nMETHOD 3: WORD2VEC (trained on your data)\n============================================================\nAccuracy: 0.8771 | F1: 0.8796\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## GloVe","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"METHOD 4: GLOVE (pretrained embeddings)\")\nprint(\"=\" * 60)\n\nimport gensim.downloader as api\n\nprint(\"Downloading GloVe model (this may take a minute)...\")\nglove_model = api.load(\"glove-wiki-gigaword-100\")\n\ndef document_vector_glove(text, model):\n    \"\"\"Average GloVe vectors for a document\"\"\"\n    tokens = tokenize(text)\n    vectors = [model[word] for word in tokens if word in model]\n    if len(vectors) > 0:\n        return np.mean(vectors, axis=0)\n    else:\n        return np.zeros(100)\n\nX_train_glove = np.vstack([document_vector_glove(text, glove_model) for text in X_train_text]).astype(np.float32)\nX_test_glove = np.vstack([document_vector_glove(text, glove_model) for text in X_test_text]).astype(np.float32)\n\nacc, f1 = train_model(X_train_glove, X_test_glove, y_train, y_test)\nprint(f\"Accuracy: {acc:.4f} | F1: {f1:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:38:33.961240Z","iopub.execute_input":"2025-10-26T04:38:33.961783Z","iopub.status.idle":"2025-10-26T04:39:28.643546Z","shell.execute_reply.started":"2025-10-26T04:38:33.961757Z","shell.execute_reply":"2025-10-26T04:39:28.642485Z"}},"outputs":[{"name":"stdout","text":"============================================================\nMETHOD 4: GLOVE (pretrained embeddings)\n============================================================\nDownloading GloVe model (this may take a minute)...\nAccuracy: 0.7896 | F1: 0.7990\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## FastText","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"METHOD 5: FASTTEXT (pretrained embeddings)\")\nprint(\"=\" * 60)\n\nprint(\"Downloading FastText model (this may take a few minutes)...\")\nfasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n\ndef document_vector_fasttext(text, model):\n    \"\"\"Average FastText vectors for a document\"\"\"\n    tokens = tokenize(text)\n    vectors = [model[word] for word in tokens if word in model]\n    if len(vectors) > 0:\n        return np.mean(vectors, axis=0)\n    else:\n        return np.zeros(300)\n\nX_train_ft = np.vstack([document_vector_fasttext(text, fasttext_model) for text in X_train_text]).astype(np.float32)\nX_test_ft = np.vstack([document_vector_fasttext(text, fasttext_model) for text in X_test_text]).astype(np.float32)\n\nacc, f1 = train_model(X_train_ft, X_test_ft, y_train, y_test)\nprint(f\"Accuracy: {acc:.4f} | F1: {f1:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:41:37.201444Z","iopub.execute_input":"2025-10-26T04:41:37.202023Z","iopub.status.idle":"2025-10-26T04:45:05.415000Z","shell.execute_reply.started":"2025-10-26T04:41:37.201996Z","shell.execute_reply":"2025-10-26T04:45:05.414108Z"}},"outputs":[{"name":"stdout","text":"============================================================\nMETHOD 5: FASTTEXT (pretrained embeddings)\n============================================================\nDownloading FastText model (this may take a few minutes)...\nAccuracy: 0.8440 | F1: 0.8404\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"SUMMARY OF RESULTS\")\nprint(\"=\" * 60)\nprint(\"\\nAll methods tested with same simple architecture:\")\nprint(\"- 1 hidden layer (128 neurons)\")\nprint(\"- 0.3 dropout\")\nprint(\"- 6 epochs\")\nprint(\"- Learning rate: 0.001\")\nprint(\"\\n\")\nprint(\"\\nNext step: Take the best embedding and try different architectures.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:47:23.530123Z","iopub.execute_input":"2025-10-26T04:47:23.530425Z","iopub.status.idle":"2025-10-26T04:47:23.536124Z","shell.execute_reply.started":"2025-10-26T04:47:23.530404Z","shell.execute_reply":"2025-10-26T04:47:23.535264Z"}},"outputs":[{"name":"stdout","text":"============================================================\nSUMMARY OF RESULTS\n============================================================\n\nAll methods tested with same simple architecture:\n- 1 hidden layer (128 neurons)\n- 0.3 dropout\n- 6 epochs\n- Learning rate: 0.001\n\nCheck the accuracy & F1 scores above to see which embedding works best!\n\nNext step: Take the best embedding and try different architectures.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Testing out different NN architectures usign Word2Vec Embeddings","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# NEURAL NETWORK IMPROVEMENT EXPERIMENTS\n\n# ============================================================================\n# DEFINE ARCHITECTURES\n# ============================================================================\nimport time\nclass BaselineNN(nn.Module):\n    \"\"\"\n    BASELINE MODEL\n    - 1 hidden layer (128 neurons)\n    - Simple dropout\n    - Total parameters: ~40K\n    \"\"\"\n    def __init__(self, input_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, 128)\n        self.dropout = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(128, 1)\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        return torch.sigmoid(self.fc2(x)).squeeze(1)\n\n\nclass DeeperNN(nn.Module):\n    \"\"\"\n    EXPERIMENT 1: DEEPER NETWORK\n    - 3 hidden layers (256 -> 128 -> 64)\n    - Batch normalization added\n    - Total parameters: ~110K\n    \n    HYPOTHESIS: More layers = better feature learning\n    \"\"\"\n    def __init__(self, input_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, 256)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.dropout1 = nn.Dropout(0.3)\n        \n        self.fc2 = nn.Linear(256, 128)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.dropout2 = nn.Dropout(0.3)\n        \n        self.fc3 = nn.Linear(128, 64)\n        self.bn3 = nn.BatchNorm1d(64)\n        self.dropout3 = nn.Dropout(0.2)\n        \n        self.fc4 = nn.Linear(64, 1)\n    \n    def forward(self, x):\n        x = torch.relu(self.bn1(self.fc1(x)))\n        x = self.dropout1(x)\n        x = torch.relu(self.bn2(self.fc2(x)))\n        x = self.dropout2(x)\n        x = torch.relu(self.bn3(self.fc3(x)))\n        x = self.dropout3(x)\n        return torch.sigmoid(self.fc4(x)).squeeze(1)\n\n\nclass WiderNN(nn.Module):\n    \"\"\"\n    EXPERIMENT 2: WIDER NETWORK\n    - 2 hidden layers (512 -> 256)\n    - More neurons per layer\n    - Total parameters: ~290K\n    \n    HYPOTHESIS: More neurons = more capacity to learn complex patterns\n    \"\"\"\n    def __init__(self, input_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.dropout1 = nn.Dropout(0.4)\n        \n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.dropout2 = nn.Dropout(0.3)\n        \n        self.fc3 = nn.Linear(256, 1)\n    \n    def forward(self, x):\n        x = torch.relu(self.bn1(self.fc1(x)))\n        x = self.dropout1(x)\n        x = torch.relu(self.bn2(self.fc2(x)))\n        x = self.dropout2(x)\n        return torch.sigmoid(self.fc3(x)).squeeze(1)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T04:57:37.254739Z","iopub.execute_input":"2025-10-26T04:57:37.255228Z","iopub.status.idle":"2025-10-26T04:57:37.272427Z","shell.execute_reply.started":"2025-10-26T04:57:37.255206Z","shell.execute_reply":"2025-10-26T04:57:37.271637Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"##  Training Function","metadata":{}},{"cell_type":"code","source":"\ndef train_model(X_train, X_test, y_train, y_test, \n                model_class, epochs, lr, batch_size, name):\n    \"\"\"Train and evaluate a model\"\"\"\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"{name}\")\n    print(f\"{'='*80}\")\n    print(f\"Hyperparameters:\")\n    print(f\"  - Epochs: {epochs}\")\n    print(f\"  - Learning Rate: {lr}\")\n    print(f\"  - Batch Size: {batch_size}\")\n    \n    start_time = time.time()\n    \n    # Prepare data\n    X_tr = torch.tensor(X_train, dtype=torch.float32)\n    X_te = torch.tensor(X_test, dtype=torch.float32)\n    y_tr = torch.tensor(y_train, dtype=torch.float32)\n    y_te = torch.tensor(y_test, dtype=torch.float32)\n    \n    # Initialize model\n    model = model_class(X_train.shape[1]).to(device)\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"  - Parameters: {num_params:,}\")\n    \n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    loss_fn = nn.BCELoss()\n    train_loader = DataLoader(TensorDataset(X_tr, y_tr), batch_size=batch_size, shuffle=True)\n    \n    # Training loop\n    print(f\"\\nTraining...\")\n    losses = []\n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0.0\n        \n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            predictions = model(X_batch)\n            loss = loss_fn(predictions, y_batch)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        \n        avg_loss = epoch_loss / len(train_loader)\n        losses.append(avg_loss)\n        \n        # Print progress\n        if epoch == 0 or (epoch + 1) % 5 == 0:\n            print(f\"  Epoch {epoch+1:2d}/{epochs} - Loss: {avg_loss:.4f}\")\n    \n    # Evaluation\n    model.eval()\n    with torch.no_grad():\n        predictions = model(X_te.to(device)).cpu().numpy()\n    y_pred = (predictions > 0.5).astype(int)\n    \n    acc = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    training_time = time.time() - start_time\n    \n    print(f\"\\n{'Results:'}\")\n    print(f\"  ✓ Accuracy: {acc:.4f}\")\n    print(f\"  ✓ F1 Score: {f1:.4f}\")\n    print(f\"  ✓ Training Time: {training_time:.2f}s\")\n    \n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'time': training_time,\n        'losses': losses,\n        'params': num_params,\n        'name': name\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T15:13:54.160724Z","iopub.execute_input":"2025-10-26T15:13:54.161297Z","iopub.status.idle":"2025-10-26T15:13:54.172354Z","shell.execute_reply.started":"2025-10-26T15:13:54.161274Z","shell.execute_reply":"2025-10-26T15:13:54.171489Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Running different tests- changing architecutre, optimizing hyperparameters","metadata":{}},{"cell_type":"markdown","source":"### simple NN architecture is considered as baseline, we see word2Vec embeddings gave better accuracy, so for simplicity using that embeddings for testing different NN models","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"RUNNING EXPERIMENTS\")\nprint(\"=\"*80)\n\nresults = []\n\n# BASELINE\nresults.append(train_model(\n    X_train_w2v, X_test_w2v, y_train, y_test,\n    model_class=BaselineNN,\n    epochs=10,\n    lr=0.001,\n    batch_size=64,\n    name=\"BASELINE: Simple Network (1 layer, 128 neurons)\"\n))\n\n# EXPERIMENT 1: Deeper Network\nresults.append(train_model(\n    X_train_w2v, X_test_w2v, y_train, y_test,\n    model_class=DeeperNN,\n    epochs=10,\n    lr=0.001,\n    batch_size=64,\n    name=\"EXPERIMENT 1: Deeper Network (3 layers: 256→128→64)\"\n))\n\n# EXPERIMENT 2: Wider Network\nresults.append(train_model(\n    X_train_w2v, X_test_w2v, y_train, y_test,\n    model_class=WiderNN,\n    epochs=10,\n    lr=0.001,\n    batch_size=64,\n    name=\"EXPERIMENT 2: Wider Network (2 layers: 512→256)\"\n))\n\n# EXPERIMENT 3: Optimized Hyperparameters (using deeper network)\nresults.append(train_model(\n    X_train_w2v, X_test_w2v, y_train, y_test,\n    model_class=DeeperNN,\n    epochs=10,  # More epochs\n    lr=0.0005,  # Lower learning rate\n    batch_size=64,\n    name=\"EXPERIMENT 3: Deeper Network + Optimized Hyperparameters (20 epochs, LR=0.0005)\"\n))\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T05:29:56.704658Z","iopub.execute_input":"2025-10-26T05:29:56.705014Z","iopub.status.idle":"2025-10-26T05:31:07.208075Z","shell.execute_reply.started":"2025-10-26T05:29:56.704995Z","shell.execute_reply":"2025-10-26T05:31:07.207153Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nRUNNING EXPERIMENTS\n================================================================================\n\n================================================================================\nBASELINE: Simple Network (1 layer, 128 neurons)\n================================================================================\nHyperparameters:\n  - Epochs: 10\n  - Learning Rate: 0.001\n  - Batch Size: 64\n  - Parameters: 38,657\n\nTraining...\n  Epoch  1/10 - Loss: 0.4591\n  Epoch  5/10 - Loss: 0.3013\n  Epoch 10/10 - Loss: 0.2883\n\nResults:\n  ✓ Accuracy: 0.8803\n  ✓ F1 Score: 0.8775\n  ✓ Training Time: 13.03s\n\n================================================================================\nEXPERIMENT 1: Deeper Network (3 layers: 256→128→64)\n================================================================================\nHyperparameters:\n  - Epochs: 10\n  - Learning Rate: 0.001\n  - Batch Size: 64\n  - Parameters: 119,169\n\nTraining...\n  Epoch  1/10 - Loss: 0.3206\n  Epoch  5/10 - Loss: 0.2532\n  Epoch 10/10 - Loss: 0.2185\n\nResults:\n  ✓ Accuracy: 0.8907\n  ✓ F1 Score: 0.8938\n  ✓ Training Time: 20.08s\n\n================================================================================\nEXPERIMENT 2: Wider Network (2 layers: 512→256)\n================================================================================\nHyperparameters:\n  - Epochs: 10\n  - Learning Rate: 0.001\n  - Batch Size: 64\n  - Parameters: 287,233\n\nTraining...\n  Epoch  1/10 - Loss: 0.3129\n  Epoch  5/10 - Loss: 0.2523\n  Epoch 10/10 - Loss: 0.2119\n\nResults:\n  ✓ Accuracy: 0.8820\n  ✓ F1 Score: 0.8767\n  ✓ Training Time: 17.06s\n\n================================================================================\nEXPERIMENT 3: Deeper Network + Optimized Hyperparameters (20 epochs, LR=0.0005)\n================================================================================\nHyperparameters:\n  - Epochs: 10\n  - Learning Rate: 0.0005\n  - Batch Size: 64\n  - Parameters: 119,169\n\nTraining...\n  Epoch  1/10 - Loss: 0.3355\n  Epoch  5/10 - Loss: 0.2529\n  Epoch 10/10 - Loss: 0.2180\n\nResults:\n  ✓ Accuracy: 0.8927\n  ✓ F1 Score: 0.8941\n  ✓ Training Time: 20.30s\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# EXPERIMENT 4: Optimized Hyperparameters (using deeper network), increased epochs\nresults.append(train_model(\n    X_train_w2v, X_test_w2v, y_train, y_test,\n    model_class=DeeperNN,\n    epochs=100,  # More epochs\n    lr=0.001,  # Lower learning rate\n    batch_size=64,\n    name=\"EXPERIMENT 4: Deeper Network + Optimized Hyperparameters (20 epochs, LR=0.0001)\"\n))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T05:31:07.209342Z","iopub.execute_input":"2025-10-26T05:31:07.209778Z","iopub.status.idle":"2025-10-26T05:34:28.211491Z","shell.execute_reply.started":"2025-10-26T05:31:07.209744Z","shell.execute_reply":"2025-10-26T05:34:28.210562Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nEXPERIMENT 3: Deeper Network + Optimized Hyperparameters (20 epochs, LR=0.0005)\n================================================================================\nHyperparameters:\n  - Epochs: 100\n  - Learning Rate: 0.001\n  - Batch Size: 64\n  - Parameters: 119,169\n\nTraining...\n  Epoch  1/100 - Loss: 0.3218\n  Epoch  5/100 - Loss: 0.2553\n  Epoch 10/100 - Loss: 0.2212\n  Epoch 15/100 - Loss: 0.1835\n  Epoch 20/100 - Loss: 0.1575\n  Epoch 25/100 - Loss: 0.1387\n  Epoch 30/100 - Loss: 0.1212\n  Epoch 35/100 - Loss: 0.1104\n  Epoch 40/100 - Loss: 0.1025\n  Epoch 45/100 - Loss: 0.0903\n  Epoch 50/100 - Loss: 0.0868\n  Epoch 55/100 - Loss: 0.0842\n  Epoch 60/100 - Loss: 0.0753\n  Epoch 65/100 - Loss: 0.0727\n  Epoch 70/100 - Loss: 0.0695\n  Epoch 75/100 - Loss: 0.0678\n  Epoch 80/100 - Loss: 0.0670\n  Epoch 85/100 - Loss: 0.0635\n  Epoch 90/100 - Loss: 0.0590\n  Epoch 95/100 - Loss: 0.0589\n  Epoch 100/100 - Loss: 0.0544\n\nResults:\n  ✓ Accuracy: 0.8802\n  ✓ F1 Score: 0.8774\n  ✓ Training Time: 200.99s\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"### Since the basic NN architecture has better accuracy, optimzied the hyperparameters like learning rate, increased epochs, increased batch size to see if there is any better performance and yes there is slight improvement from 87% to 89%","metadata":{}},{"cell_type":"code","source":"# experiment 5 changed batch size and epochs as 100, learning rate as it is\nresults.append(train_model(\n    X_train_w2v, X_test_w2v, y_train, y_test,\n    model_class=BaselineNN,\n    epochs=100,\n    lr=0.001,\n    batch_size=128,\n    name=\"experiment 5: Simple Network (1 layer, 128 neurons) with tuned hyper parameters\"\n))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:11:12.091068Z","iopub.execute_input":"2025-10-26T06:11:12.091644Z","iopub.status.idle":"2025-10-26T06:12:32.918883Z","shell.execute_reply.started":"2025-10-26T06:11:12.091613Z","shell.execute_reply":"2025-10-26T06:12:32.918238Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nBASELINE: Simple Network (1 layer, 128 neurons)\n================================================================================\nHyperparameters:\n  - Epochs: 100\n  - Learning Rate: 0.001\n  - Batch Size: 128\n  - Parameters: 38,657\n\nTraining...\n  Epoch  1/100 - Loss: 0.5206\n  Epoch  5/100 - Loss: 0.3079\n  Epoch 10/100 - Loss: 0.2938\n  Epoch 15/100 - Loss: 0.2879\n  Epoch 20/100 - Loss: 0.2841\n  Epoch 25/100 - Loss: 0.2795\n  Epoch 30/100 - Loss: 0.2770\n  Epoch 35/100 - Loss: 0.2749\n  Epoch 40/100 - Loss: 0.2733\n  Epoch 45/100 - Loss: 0.2696\n  Epoch 50/100 - Loss: 0.2678\n  Epoch 55/100 - Loss: 0.2662\n  Epoch 60/100 - Loss: 0.2659\n  Epoch 65/100 - Loss: 0.2638\n  Epoch 70/100 - Loss: 0.2624\n  Epoch 75/100 - Loss: 0.2607\n  Epoch 80/100 - Loss: 0.2588\n  Epoch 85/100 - Loss: 0.2553\n  Epoch 90/100 - Loss: 0.2559\n  Epoch 95/100 - Loss: 0.2558\n  Epoch 100/100 - Loss: 0.2535\n\nResults:\n  ✓ Accuracy: 0.8900\n  ✓ F1 Score: 0.8910\n  ✓ Training Time: 80.82s\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"###  and accuracy has slightly improved","metadata":{}},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# RESULTS SUMMARY\n# ============================================================================\nprint(\"\\n\\n\" + \"=\"*80)\nprint(\"FINAL RESULTS SUMMARY\")\nprint(\"=\"*80)\n\n# Create comparison table\nprint(\"\\n{:<60} {:>10} {:>10} {:>12} {:>10}\".format(\n    \"Experiment\", \"Accuracy\", \"F1 Score\", \"Parameters\", \"Time (s)\"\n))\nprint(\"-\" * 104)\n\nfor i, r in enumerate(results):\n    exp_name = \"BASELINE\" if i == 0 else f\"EXP {i}\"\n    print(\"{:<60} {:>10.4f} {:>10.4f} {:>12,} {:>10.1f}\".format(\n        exp_name, r['accuracy'], r['f1'], r['params'], r['time']\n    ))\n\n# Calculate improvements\nprint(\"\\n\" + \"=\"*80)\nprint(\"IMPROVEMENTS OVER BASELINE\")\nprint(\"=\"*80)\n\nbaseline = results[0]\nfor i in range(1, len(results)):\n    r = results[i]\n    acc_imp = ((r['accuracy'] - baseline['accuracy']) / baseline['accuracy']) * 100\n    f1_imp = ((r['f1'] - baseline['f1']) / baseline['f1']) * 100\n    \n    print(f\"\\nEXPERIMENT {i}:\")\n    print(f\"  Accuracy: {acc_imp:+.2f}% {'✓' if acc_imp > 0 else '✗'}\")\n    print(f\"  F1 Score: {f1_imp:+.2f}% {'✓' if f1_imp > 0 else '✗'}\")\n    print(f\"  Extra Time: +{r['time'] - baseline['time']:.1f}s\")\n\n# Find best model\nbest = max(results, key=lambda x: x['f1'])\nbest_idx = results.index(best)\nprint(f\"\\n BEST MODEL: {'BASELINE' if best_idx == 0 else f'EXPERIMENT {best_idx}'}\")\nprint(f\"   Accuracy: {best['accuracy']:.4f}\")\nprint(f\"   F1 Score: {best['f1']:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualization of F1 scores, training loss","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nprint(\"\\n\" + \"=\"*80)\nprint(\"CREATING VISUALIZATIONS\")\nprint(\"=\"*80)\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle('Neural Network Architecture Experiments - Results Comparison', \n             fontsize=16, fontweight='bold')\n\nnames = ['BASELINE', 'EXP 1\\n(Deeper)', 'EXP 2\\n(Wider)', 'EXP 3\\n(Optimized)', 'EXP4','EXP5(baseline optimized)']\ncolors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4','#FFB347','#CDB4DB']\n\n# Plot 1: Accuracy Comparison\nax = axes[0, 0]\naccuracies = [r['accuracy'] for r in results]\nbars = ax.bar(names, accuracies, color=colors, edgecolor='black', linewidth=1.5, alpha=0.8)\nax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\nax.set_title('Accuracy Comparison', fontsize=13, fontweight='bold')\nax.set_ylim([min(accuracies) - 0.01, max(accuracies) + 0.01])\nax.axhline(y=baseline['accuracy'], color='red', linestyle='--', linewidth=2, label='Baseline')\nax.grid(axis='y', alpha=0.3, linestyle='--')\nax.legend()\n\n# Add value labels\nfor bar in bars:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n\n# Plot 2: F1 Score Comparison\nax = axes[0, 1]\nf1_scores = [r['f1'] for r in results]\nbars = ax.bar(names, f1_scores, color=colors, edgecolor='black', linewidth=1.5, alpha=0.8)\nax.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\nax.set_title('F1 Score Comparison', fontsize=13, fontweight='bold')\nax.set_ylim([min(f1_scores) - 0.01, max(f1_scores) + 0.01])\nax.axhline(y=baseline['f1'], color='red', linestyle='--', linewidth=2, label='Baseline')\nax.grid(axis='y', alpha=0.3, linestyle='--')\nax.legend()\n\n# Add value labels\nfor bar in bars:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n\n# Plot 3: Training Loss Curves\nax = axes[1, 0]\nfor i, r in enumerate(results):\n    ax.plot(r['losses'], label=names[i], marker='o', linewidth=2, \n            markersize=4, color=colors[i])\nax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\nax.set_ylabel('Loss', fontsize=12, fontweight='bold')\nax.set_title('Training Loss Over Epochs', fontsize=13, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3, linestyle='--')\n\n# Plot 4: Model Complexity vs Performance\nax = axes[1, 1]\nparams = [r['params'] for r in results]\nscatter = ax.scatter(params, f1_scores, s=200, c=colors, edgecolor='black', \n                     linewidth=2, alpha=0.8, zorder=3)\nax.set_xlabel('Number of Parameters', fontsize=12, fontweight='bold')\nax.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\nax.set_title('Model Complexity vs Performance', fontsize=13, fontweight='bold')\nax.grid(True, alpha=0.3, linestyle='--')\n\n# Add labels for each point\nfor i, (x, y) in enumerate(zip(params, f1_scores)):\n    ax.annotate(names[i], (x, y), xytext=(5, 5), textcoords='offset points',\n                fontweight='bold', fontsize=9)\n\nplt.tight_layout()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}